Example 1:
ğ—§ğ—µğ—² ğ—˜ğ—»ğ—± ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—¥ğ—®ğ—°ğ—² ğ—³ğ—¼ğ—¿ ğ—Ÿğ—®ğ—¿ğ—´ğ—²ğ—¿ ğ—™ğ—¿ğ—¼ğ—»ğ˜ğ—¶ğ—²ğ—¿ ğ—”ğ—œ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€â“

ğ—œğ—»ğ˜ğ—²ğ—¿ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ˜ğ—®ğ—¸ğ—² ğ—¯ğ˜† ğ—œğ—¹ğ˜†ğ—® ğ—¦ğ˜‚ğ˜ğ˜€ğ—¸ğ—²ğ˜ƒğ—²ğ—¿ ğ—¼ğ—» ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´ ğ—Ÿğ—®ğ˜„ğ˜€ ğ—¶ğ—» ğ—Ÿğ—Ÿğ—  ğ—£ğ—¿ğ—²-ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğŸ¤”: 

In his talk at NeurIPS Conference, OpenAI's former Chief Scientist shared an interesting perspective. It challenges the 2020 Scaling Laws paper, which has set the direction for LLM pre-training over the last years.

ğ—¥ğ—²ğ—°ğ—®ğ—½ ğ—¼ğ—» ğ˜ğ—µğ—² ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´ ğ—Ÿğ—®ğ˜„ğ˜€ ğ—£ğ—®ğ—½ğ—²ğ—¿:
â€¢ ğŸ“ˆ ğ—£ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ğ—®ğ—¯ğ—¹ğ—² ğ—œğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€: As model size (=number of parameters), data (=used for pre-training), and compute (=performance of GPU clusters) grow, LLM performance improves in smooth, predictable patterns.
â€¢ ğŸ¯ ğ—ğ—²ğ˜† ğ—™ğ—¶ğ—»ğ—±ğ—¶ğ—»ğ—´: Larger models trained on larger datasets and clusters show systematic, steady gainsâ€”a clear roadmap.

ğ—¡ğ—¼ğ˜„, ğ—¦ğ˜‚ğ˜ğ˜€ğ—¸ğ—²ğ˜ƒğ—²ğ—¿â€™ğ˜€ ğ—£ğ—²ğ—¿ğ˜€ğ—½ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—²:
â³ ğ——ğ—®ğ˜ğ—® ğ—•ğ—¼ğ˜ğ˜ğ—¹ğ—²ğ—»ğ—²ğ—°ğ—¸: Simply adding more internet-like text will soon hit a wall; available data wonâ€™t keep up with LLM pre-training demands.
ğŸš€ ğ—¡ğ—²ğ˜…ğ˜ ğ—¦ğ˜ğ—²ğ—½ğ˜€: Beyond scaling up, the future involves new approaches:
â€¢ ğ—¦ğ˜†ğ—»ğ˜ğ—µğ—²ğ˜ğ—¶ğ—° ğ——ğ—®ğ˜ğ—®: Curating/creating richer, high-quality datasets in-house.
â€¢ ğ—”ğ—´ğ—²ğ—»ğ˜ğ˜€: Systems that actively reason and explore, not just passively consume data.
â€¢ ğ——ğ˜†ğ—»ğ—®ğ—ºğ—¶ğ—° ğ—”ğ—±ğ—®ğ—½ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»: LLMs that refine themselves continuously, learning from smaller but more relevant data streams.

ğ—–ğ—¼ğ—»ğ—°ğ—¹ğ˜‚ğ˜€ğ—¶ğ—¼ğ—»:
â€¢ Scaling laws made progress straightforwardâ€”larger models trained on more data and better clusters were simply better. 
â€¢ Due to the challenging data situation, we might enter an era, where new techniques will need to drive the next wave of AI improvements.

#AI #ScalingLaws #LLM #OpenAI #Sutskever

Example 2:
ğ—ªğ—µğ—®ğ˜ ğ—® ğ—¬ğ—²ğ—®ğ—¿ ğŸ®ğŸ¬ğŸ®ğŸ° ğ—³ğ—¼ğ—¿ ğ—”ğ—œ ğŸ¤– ... ğ—¥ğ—²ğ—®ğ—±ğ˜† ğ—³ğ—¼ğ—¿ ğ—® ğ—¥ğ—²ğ—°ğ—®ğ—½â“

ğ—›ğ—²ğ—¿ğ—² ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—§ğ—¼ğ—½ ğŸ­ğŸ± ğ—›ğ—²ğ—®ğ—±ğ—¹ğ—¶ğ—»ğ—²ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—¶ğ—°ğ—¶ğ—®ğ—¹ğ—”ğ—»ğ—®ğ—¹ğ˜†ğ˜€ğ—¶ğ˜€.ğ—®ğ—¶â€™ğ˜€ ğŸ®ğŸ¬ğŸ®ğŸ° ğ—¥ğ—²ğ˜ƒğ—¶ğ—²ğ˜„ âœ¨:

ğŸ­/ ğ—•ğ—®ğ˜ğ˜ğ—¹ğ—² ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—™ğ—¿ğ—¼ğ—»ğ˜ğ—¶ğ—²ğ—¿ ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€: Multiple AI labs caught up to GPT-4â€™s intelligence in 2024 and some models surpassed it.
ğŸ®/ ğ—¨ğ—¦ ğ—¶ğ—» ğ˜ğ—µğ—² ğ—Ÿğ—²ğ—®ğ—±: The USA leads the AI frontier; China in second place, with few others also at the cutting edge.
ğŸ¯/ ğ—¢ğ—½ğ—²ğ—»-ğ—¦ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—–ğ—®ğ˜ğ—°ğ—µ-ğ˜‚ğ—½: Open-source models from Meta, Mistral, and Alibaba significantly narrowed the gap to proprietary models.
ğŸ°/ ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¿ğ—¶ğ—°ğ—² ğ——ğ—¿ğ—¼ğ—½: Inference costs plunged in 2024, as GPT-4o mini came close to GPT-4â€™s intelligence at 1% the price.
ğŸ±/ ğ—¦ğ—ºğ—®ğ—¹ğ—¹ ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€ (ğ—¦ğ—Ÿğ— ğ˜€) ğ—¼ğ—» ğ˜ğ—µğ—² ğ—¥ğ—¶ğ˜€ğ—²: Smaller models now match large-model intelligence, driving down costs and increasing speed.

ğŸ²/ ğ—Ÿğ—®ğ—¿ğ—´ğ—²ğ—¿ ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—ªğ—¶ğ—»ğ—±ğ—¼ğ˜„ğ˜€: Context windows expanded to 128k tokens as a new standard, enabling richer long-form reasoning.
ğŸ³/ ğ—©ğ—²ğ—¿ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—œğ—»ğ˜ğ—²ğ—´ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»: Vertical integration varies across AI players; cloud providers like AWS stand out with offerings across the stack.
ğŸ´/ ğ—™ğ—¿ğ—¼ğ—»ğ˜ğ—¶ğ—²ğ—¿ ğ—Ÿğ—®ğ—¯ğ˜€ ğ—–ğ—®ğ—½ğ˜ğ˜‚ğ—¿ğ—² ğ——ğ—²ğ—ºğ—®ğ—»ğ—±: Demand focuses on top AI labs as reasoning quality and price guide model selection.
ğŸµ/ ğ—¨ğ˜€ğ—² ğ—–ğ—®ğ˜€ğ—²ğ˜€ ğ—®ğ—¿ğ—² ğ——ğ—¶ğ˜ƒğ—²ğ—¿ğ˜€ğ—²: Companies apply LLMs in many different ways with multimodal capabilities on most roadmaps.
ğŸ­ğŸ¬/ ğ— ğ˜‚ğ—¹ğ˜ğ—¶-ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—”ğ—½ğ—½ğ—¿ğ—¼ğ—®ğ—°ğ—µ: Most users run multiple models, with about 75% using hosted serverless endpoints, such as AWS Bedrock.

ğŸ­ğŸ­/ ğ—œğ—ºğ—®ğ—´ğ—² ğ—¤ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜† ğ—œğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€: Image generation quality soared in 2024, improving photorealism and prompt adherence.
ğŸ­ğŸ®/ ğ—•ğ—®ğ˜ğ˜ğ—¹ğ—² ğ—¶ğ—» ğ˜ğ—µğ—² ğ—œğ—ºğ—®ğ—´ğ—² ğ—”ğ—¿ğ—²ğ—»ğ—®: Intense competition emerged between image models; top 5 players all launched post-Q3 2024.
ğŸ­ğŸ¯/ ğ—¦ğ—¼ğ—¿ğ—® ğ—Ÿğ—®ğ˜ğ—² ğ˜ğ—¼ ğ˜ğ—µğ—² ğ—£ğ—®ğ—¿ğ˜ğ˜†: OpenAIâ€™s Sora went from no competition during its preview in Feb. to a crowded market at launch in Dec.
ğŸ­ğŸ°/ ğ—§ğ—²ğ˜…ğ˜-ğ˜ğ—¼-ğ—¦ğ—½ğ—²ğ—²ğ—°ğ—µ ğ—œğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€: Transformer-based Text-to-Speech leaped ahead in quality, outpacing established leaders.
ğŸ­ğŸ±/ ğ—¢ğ—½ğ—²ğ—»-ğ—¦ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—šğ—®ğ—¶ğ—»ğ˜€ ğ—¶ğ—» ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—°ğ—¿ğ—¶ğ—½ğ˜ğ—¶ğ—¼ğ—»: OpenAIâ€™s Whisper sparked competition in AI transcription, enabling new speed and price battles.

Note: The attached report includes one detailed slide per headline. 

#AI #ArtificialAnalysis #2024 #Recap